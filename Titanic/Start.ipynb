{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost= 0.691702276468277\n",
      "Epoch: 2 cost= 0.6882063647111257\n",
      "Epoch: 3 cost= 0.6847956577936808\n",
      "Epoch: 4 cost= 0.6814670066038768\n",
      "Epoch: 5 cost= 0.6782173613707224\n",
      "Epoch: 6 cost= 0.6750437319278717\n",
      "Epoch: 7 cost= 0.6719434261322021\n",
      "Epoch: 8 cost= 0.6689137419064839\n",
      "Epoch: 9 cost= 0.665952205657959\n",
      "Epoch: 10 cost= 0.6630563735961914\n",
      "Epoch: 11 cost= 0.6602240105470021\n",
      "Epoch: 12 cost= 0.6574528912703196\n",
      "Epoch: 13 cost= 0.6547409892082214\n",
      "Epoch: 14 cost= 0.6520861983299255\n",
      "Epoch: 15 cost= 0.6494867702325186\n",
      "Accuracy: 0.811321\n",
      "WARNING:tensorflow:From <ipython-input-1-a8d19b1fd6b7>:123: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "# Using code I found online as reference\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCH_NUM = 15\n",
    "BATCH_SIZE = 100\n",
    "LOGS_PATH = '/tmp/tensorflow_logs'\n",
    "\n",
    "\n",
    "def preprocess_data(path, is_test=False):\n",
    "    data = pd.read_csv(path, index_col='PassengerId')\n",
    "    data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "    if is_test:\n",
    "        data = data.replace([None], [0])\n",
    "    else:\n",
    "        data = data[pd.notnull(data['Age'])]\n",
    "        data = data[pd.notnull(data['Embarked'])]\n",
    "    data.replace([\"female\", \"male\"], [0, 1], inplace=True)\n",
    "    data.replace([\"Q\", \"C\", \"S\"], [0, 1, 2], inplace=True)\n",
    "    if \"Survived\" in data:\n",
    "        data = data[pd.notnull(data['Survived'])]\n",
    "    data_norm = (data - data.mean()) / (data.max() - data.min())\n",
    "    return data_norm\n",
    "\n",
    "\n",
    "def next_batch(df, i=None):\n",
    "    \"\"\"\n",
    "    :param df: pandas dataframe\n",
    "    :param i: batch index\n",
    "    :return: (numpy array x, numpy array y)\n",
    "    \"\"\"\n",
    "    if i is None:\n",
    "        start = 0\n",
    "        end = df.shape[0]\n",
    "    else:\n",
    "        start = BATCH_SIZE * i\n",
    "        end = BATCH_SIZE * (i + 1)\n",
    "    result = df[start:end]\n",
    "    if \"Survived\" in result:\n",
    "        batch_ys = pd.get_dummies(result.pop('Survived').values).as_matrix()\n",
    "        batch_xs = result.as_matrix()\n",
    "        return batch_xs, batch_ys\n",
    "    else:\n",
    "        return result.as_matrix()\n",
    "\n",
    "\n",
    "def split_dataset(df, test_part=None):\n",
    "    \"\"\"\n",
    "    Split dataframe\n",
    "    :param test_part: float from 0 to 1\n",
    "    :param df: pandas dataframe\n",
    "    :return: (pandas dataframe train, pandas dataframe test)\n",
    "    \"\"\"\n",
    "    length = df.shape[0]\n",
    "    if test_part is None:\n",
    "        test_part = 0.15\n",
    "\n",
    "    test_part = int(length * test_part)\n",
    "\n",
    "    test_dataset = df[0:test_part]\n",
    "    training_dataset = df[test_part:]\n",
    "    return training_dataset, test_dataset\n",
    "\n",
    "\n",
    "dataset = preprocess_data(TRAIN_PATH)\n",
    "\n",
    "training_dataset, test_narray = split_dataset(dataset)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 7], name='InputData')\n",
    "y = tf.placeholder(tf.float32, [None, 2], name='TargetData')\n",
    "\n",
    "W = tf.Variable(tf.zeros([7, 2]), name='Weights')\n",
    "b = tf.Variable(tf.zeros([2]), name='Bias')\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    pred = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred + 1e-10), reduction_indices=1))\n",
    "\n",
    "with tf.name_scope('GDS'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)\n",
    "\n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "tf.summary.scalar(\"accuracy\", acc)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    log_writer = tf.summary.FileWriter(LOGS_PATH, graph=tf.get_default_graph())\n",
    "    training_dataset_size = training_dataset.shape[0]\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(training_dataset_size / BATCH_SIZE)\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = next_batch(training_dataset, i)\n",
    "            _, c, summary = sess.run([optimizer, cost, merged_summary], feed_dict={x: batch_xs, y: batch_ys})\n",
    "            log_writer.add_summary(summary, epoch * total_batch + i)\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print(\"Epoch:\", '%d' % (epoch + 1), \"cost=\", \"{0}\".format(avg_cost))\n",
    "\n",
    "    test_x, test_y = next_batch(test_narray)\n",
    "    print(\"Accuracy:\", acc.eval({x: test_x, y: test_y}))\n",
    "\n",
    "    test_df = preprocess_data(TEST_PATH, is_test=True)\n",
    "    indexes = test_df.index.values\n",
    "    test_narray = next_batch(test_df)\n",
    "    feed_dict = {x: test_narray}\n",
    "    predict_proba = pred.eval(feed_dict)\n",
    "    predictions = tf.argmax(predict_proba, dimension=1).eval()\n",
    "\n",
    "    with open(\"kaggle.csv\", \"w\") as f:\n",
    "        f.write(\"PassengerId,Survived\\n\")\n",
    "        for index, prediction in zip(indexes, predictions):\n",
    "            f.write(\"{0},{1}\\n\".format(index, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
